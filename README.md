# Bank Multi-Agent Expert System

[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com/)
[![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-purple.svg)](https://github.com/langchain-ai/langgraph)

High-efficiency multi-agent system with **KV Cache Optimization** for querying the Internal Operations & Compliance Manual (~25,000 tokens).

## ğŸ¯ Key Features

| Feature | Description |
|---------|-------------|
| **Prefix Caching** | 25k-token manual loaded once, cached for all agents via LMCache |
| **Parallel Execution** | Fan-out to multiple specialized agents using LangGraph |
| **TTFT Optimization** | Inferred cache hit rate from response times |
| **Langfuse Observability** | Full tracing and monitoring from day one |
| **Prompty Templates** | All prompts defined in `.prompty` files for maintainability |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOCAL MACHINE (FastAPI + LangGraph Orchestration)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  â€¢ DeterministicPromptBuilder (whitespace norm)     â”‚    â”‚
â”‚  â”‚  â€¢ CacheAwareMetrics (TTFT tracking)                â”‚    â”‚
â”‚  â”‚  â€¢ Langfuse integration                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ HTTP (OpenAI API)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REMOTE vLLM SERVER (GPU)                                   â”‚
â”‚  â€¢ LMCache with CacheBlend                                  â”‚
â”‚  â€¢ Prefix caching for shared manual                         â”‚
â”‚  â€¢ Qwen/Qwen3-30B-A3B-Instruct-2507                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow

```
START â†’ Router â†’ Parallel Agents â†’ Aggregator â†’ END
```

The **Router** classifies queries and selects 1-3 agents. All agents share the same 25k-token manual prefix, maximizing KV cache hits.

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Orchestration** | LangGraph 0.2+ |
| **LLM Framework** | LangChain + LangChain-OpenAI |
| **API** | FastAPI 0.115+ |
| **Prompt Management** | Prompty |
| **Observability** | Langfuse |
| **LLM Backend** | Remote vLLM with LMCache |
| **Package Manager** | uv |

## ğŸ“‹ Prerequisites

- **Python 3.13+**
- **uv** package manager ([install guide](https://docs.astral.sh/uv/))
- Access to remote vLLM endpoint (or local GPU setup)
- Langfuse account for observability

## ğŸš€ Quick Start

```bash
# 1. Clone and navigate to project
cd test_tensormesh

# 2. Install dependencies
uv sync

# 3. Configure environment
cp .env.example .env
# Edit .env with your API keys and endpoint

# 4. Run the server
uv run uvicorn src.main:app --reload

# 5. Test a query
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the daily ATM withdrawal limit?", "session_id": "test-123"}'
```

## ğŸ“ Project Structure

```
test_tensormesh/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app with cache warming
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py            # REST endpoints
â”‚   â”‚   â””â”€â”€ schemas.py           # Pydantic request/response models
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py          # Pydantic settings management
â”‚   â”‚   â””â”€â”€ langfuse.py          # Langfuse observability setup
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ metrics.py           # CacheAwareMetrics for TTFT tracking
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ manager.py           # DeterministicPromptBuilder
â”‚   â”‚   â”œâ”€â”€ router.prompty       # Router agent prompt
â”‚   â”‚   â”œâ”€â”€ technical_specialist.prompty
â”‚   â”‚   â”œâ”€â”€ compliance_auditor.prompty
â”‚   â”‚   â”œâ”€â”€ support_concierge.prompty
â”‚   â”‚   â”œâ”€â”€ aggregator.prompty   # Response aggregation prompt
â”‚   â”‚   â””â”€â”€ warmup.prompty       # Cache warming prompt
â”‚   â””â”€â”€ graph/
â”‚       â”œâ”€â”€ builder.py           # LangGraph workflow builder
â”‚       â”œâ”€â”€ nodes.py             # Node implementations
â”‚       â””â”€â”€ state.py             # AgentState TypedDict
â”œâ”€â”€ data/
â”‚   â””â”€â”€ operations_manual.txt    # 25k-token bank operations manual
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py              # Pytest fixtures
â”‚   â””â”€â”€ test_cache_efficiency.py # Cache optimization tests
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ pyproject.toml               # Dependencies and project config
â””â”€â”€ AGENTS.MD                    # Development guidelines
```

## ğŸ”Œ API Endpoints

### `POST /api/v1/query`

Process a user query through the multi-agent workflow.

**Request:**
```json
{
  "query": "What is the daily ATM withdrawal limit?",
  "session_id": "user-session-123",
  "user_id": "optional-user-id"
}
```

**Response:**
```json
{
  "response": "According to Section 3.2 of the manual...",
  "agents_used": ["technical_specialist", "compliance_auditor"],
  "compliance_passed": true,
  "retry_count": 0,
  "ttft_seconds": 2.34
}
```

### `GET /health`

Health check endpoint.

```bash
curl http://localhost:8000/health
```

### `GET /cache/stats`

Cache efficiency metrics for monitoring.

```bash
curl http://localhost:8000/cache/stats
```

**Response:**
```json
{
  "total_requests": 100,
  "inferred_cache_hit_rate": 0.85,
  "prefix_alignment_ok": true,
  "grade": "A - Excellent cache efficiency"
}
```

## ğŸ”§ Configuration

All settings are managed via environment variables. Copy `.env.example` to `.env` and configure:

| Variable | Description | Default |
|----------|-------------|---------|
| `VLLM_BASE_URL` | Remote vLLM endpoint | `http://89.169.108.198:30080/v1` |
| `VLLM_API_KEY` | API key for vLLM | - |
| `VLLM_MODEL` | Model name | `Qwen/Qwen3-30B-A3B-Instruct-2507` |
| `LANGFUSE_PUBLIC_KEY` | Langfuse public key | - |
| `LANGFUSE_SECRET_KEY` | Langfuse secret key | - |
| `LANGFUSE_BASE_URL` | Langfuse endpoint | `https://us.cloud.langfuse.com` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `API_HOST` | API host | `0.0.0.0` |
| `API_PORT` | API port | `8000` |
| `MANUAL_PATH` | Path to operations manual | `data/operations_manual.txt` |

## ğŸ“Š Cache Efficiency

### Optimization Strategy

| Optimization | Implementation | Expected Impact |
|--------------|----------------|-----------------|
| Deterministic prompts | Whitespace normalization, Unix line endings | 100% prefix match |
| Parallel batching | Fire all agent requests simultaneously | Max cache reuse |
| Startup warming | Pre-load manual on app start via `warmup.prompty` | Fast first request |
| TTFT tracking | Infer cache hits from response time | Visibility |

### Expected Performance

| Metric | Cold Cache | Warm Cache | Improvement |
|--------|------------|------------|-------------|
| TTFT | ~10-30s | ~1-5s | **3-10x faster** |
| Cache Hit Rate | 0% | >80% | - |

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest tests/ -v

# Run cache efficiency tests only
uv run pytest tests/test_cache_efficiency.py -v
```

### Test Coverage

- **Prefix alignment tests**: Verify all agents produce identical prefix hashes
- **TTFT inference tests**: Validate cache hit detection from response times
- **Whitespace normalization**: Ensure consistent prompt formatting

## ğŸ›ï¸ Key Components

| Component | Purpose |
|-----------|---------|
| `DeterministicPromptBuilder` | Ensures byte-identical prompts for cache hits |
| `CacheAwareMetrics` | Tracks TTFT and infers cache efficiency |
| `AgentState` | LangGraph state with Annotated reducers for parallel execution |
| `build_graph()` | Constructs the LangGraph workflow with checkpointer support |

## ğŸ“ License

Internal use only.
